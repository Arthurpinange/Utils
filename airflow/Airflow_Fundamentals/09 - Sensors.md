## Sensores

Sensores s√£o utilizados para aguardar at√© uma a√ß√£o ocorrer, antes de ir para a pr√≥xima tarefa.
Verificar o script [sensor.py](./dags/sensor_and_bashOperator.py) para ver como deve ser desenvolvido.

Notas
  1. Na interface, selecionar Admin -> Connections
     - Os dados s√£o encriptados no banco, mas visualizados na interface.
  2. O campo *Connection id* deve ter o mesmo valor informado no script.
  3. No caso de arquivos de sistema, a *connection type* deve ser *File(path)*
  4. Extra: Espera um JSON. No caso do sistema de arquivos:

    {"path":"/tmp"}
   
  5. Por padr√£o, os sensores avaliam as conex√µes a cada 30 segundos, para alterar esse valor, deve-se usar o par√¢metro *poke_interval*

Airflow traz diferentes sensores, aqui est√° uma lista n√£o exaustiva dos mais usados:

- O FileSensor : Aguarda que um arquivo ou pasta chegue em um sistema de arquivos.
- O S3KeySensor : espera que uma chave esteja presente em um bucket do S3.
- O SqlSensor : Executa uma instru√ß√£o sql repetidamente at√© que um crit√©rio seja atendido.
- O HivePartitionSensor : Aguarda que uma parti√ß√£o apare√ßa no Hive.
- O ExternalTaskSensor : aguarda a conclus√£o de um DAG diferente ou uma tarefa em um DAG diferente para uma data de execu√ß√£o espec√≠fica. (Muito √∫til esse ü§ì)
- O DateTimeSensor : Aguarda at√© a data e hora especificada (√∫til para adicionar algum atraso aos seus DAGs)
- O TimeDeltaSensor : Aguarda um timedelta ap√≥s a execu√ß√£o da tarefa + intervalo de agendamento (parece semelhante ao anterior, n√£o?)

### Tempo limite do sensor de airflow
Sempre, Sempre defina um tempo limite significativo para seus sensores. Sempre. Basicamente, ele deve ser menor que o intervalo de agendamento do seu DAG e corresponde ao intervalo de tempo no qual voc√™ espera ver sua condi√ß√£o ser atendida. Por exemplo, se seu DAG for executado uma vez por dia √†s 8h e seus arquivos devem chegar √†s 08h10, um tempo limite definido como 30 minutos pode ser uma boa ideia. Isso evitar√° o problema do Sensor Deadlock.

``` bash
waiting_for_file = FileSensor(
task_id='waiting_for_file',
poke_interval=30,
timeout=60 * 30
)
```

### Modos de Sensor
Um sensor tem dois modos. ‚Äúpoke‚Äù, que √© o modo padr√£o, significa que quando um sensor √© executado, ele ocupa um slot durante todo o tempo de execu√ß√£o e dorme entre os pokes. 

Se voc√™ configurar seu sensor para o modo ‚Äúrescheduler‚Äù, ele liberar√° o slot quando a condi√ß√£o/crit√©rio n√£o for atendido e ser√° reprogramado posteriormente. 

### E se um sensor de fluxo de ar expirar
√â √≥timo saber que voc√™ pode definir um tempo limite para seus sensores, mas seria melhor se voc√™ soubesse como agir em seguida. Bem, boas not√≠cias! Voc√™ pode definir uma fun√ß√£o de retorno de chamada caso um operador falhe. Deixe-me mostrar-lhe um exemplo:
``` bash
def _failure_callback(context):
  if isinstance(context['exception'], AirflowSensorTimeout):
  print(context)
  print("Sensor timed out")
with DAG(...) as dag:
  waiting_for_file = FileSensor(
  task_id='waiting_for_file',
  poke_interval=120,
  timeout=10,
  mode="reschedule",
  on_failure_callback=_failure_callback
  )
``` 
No c√≥digo acima, assim que seu FileSensor expirar, a fun√ß√£o _failure_callback ser√° chamada com o objeto de contexto da inst√¢ncia da tarefa fornecido no par√¢metro. Este objeto de contexto cont√©m muitas informa√ß√µes √∫teis sobre seu dag, tarefa etc. Nesse caso, voc√™ pode verificar a chave ‚Äúexce√ß√£o‚Äù e filtr√°-la. Se a exce√ß√£o for igual a AirflowSensorTimeout, seu Sensor expirou, portanto, fa√ßa o que quiser em seguida.

### Falha suave ou pule seu sensor
Uma vez que o tempo limite ocorre, voc√™ tem a op√ß√£o de marcar o sensor como com falha ou como ignorado.

Escolher um ou outro realmente depende do seu caso de uso. Por exemplo, vamos ter v√°rios sensores esperando por arquivos diferentes e se um arquivo n√£o estiver dispon√≠vel voc√™ ainda deseja processar os outros, pular esse sensor pode ser uma boa ideia, pois n√£o √© uma ‚Äúfalha‚Äù real, n√£o √© cr√≠tico para seu DAG.

Aqui est√° um exemplo completo:

``` bash
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.sensors.python import PythonSensor
from airflow.exceptions import AirflowSensorTimeout
from datetime import datetime
default_args = {
'start_date': datetime(2021, 1, 1)
}
def _done():
pass
def _partner_a():
return False
def _partner_b():
return True
def _failure_callback(context):
if isinstance(context['exception'], AirflowSensorTimeout):
print(context)
print("Sensor timed out")
with DAG('my_dag', schedule_interval='@daily', default_args=default_args, catchup=False) as dag:
partner_a = PythonSensor(
task_id='partner_a',
poke_interval=120,
timeout=10,
mode="reschedule",
python_callable=_partner_a,
on_failure_callback=_failure_callback,
soft_fail=True
)
partner_b = PythonSensor(
task_id='partner_b',
poke_interval=120,
timeout=10,
mode="reschedule",
python_callable=_partner_b,
on_failure_callback=_failure_callback,
soft_fail=True
)
done = PythonOperator(
task_id="done",
python_callable=_done,
trigger_rule='none_failed_or_skipped'
)
[partner_a, partner_b] >> done
```

temos:
<img src="img/dag_soft_fail.webp">
Observe a regra de gatilho da tarefa feita para dizer que todos os pais n√£o falharam e pelo menos um dos pais foi bem-sucedido.


