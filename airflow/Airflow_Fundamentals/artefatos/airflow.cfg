#
# Licenciado para a Apache Software Foundation (ASF) sob um
# ou mais contratos de licença de contribuidor. Veja o arquivo AVISO
# distribuído com este trabalho para informações adicionais
# sobre propriedade de direitos autorais. A ASF licencia este arquivo
# para você sob a Licença Apache, Versão 2.0 (o
# "Licença"); você não pode usar este arquivo, exceto em conformidade
# com a Licença. Você pode obter uma cópia da Licença em
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# A menos que exigido pela lei aplicável ou acordado por escrito,
# software distribuído sob a Licença é distribuído em um
# BASE "COMO ESTÁ", SEM GARANTIAS OU CONDIÇÕES DE QUALQUER
# KIND, seja expresso ou implícito. Consulte a Licença para o
# idioma específico que rege as permissões e limitações
# sob a Licença.


# Este é o modelo para a configuração padrão do Airflow. Quando o fluxo de ar é
# importado, ele procura um arquivo de configuração em $AIRFLOW_HOME/airflow.cfg. Se
# não existe, o Airflow usa este modelo para gerá-lo substituindo
# variáveis ​​entre chaves com seus valores globais de configuration.py.

# Os usuários não devem modificar este arquivo; eles devem personalizar o gerado
# airflow.cfg em vez disso.


# ----------------------- O MODELO COMEÇA AQUI -----------------------

[essencial]
# A pasta onde estão seus pipelines de fluxo de ar, provavelmente um
# subpasta em um repositório de código. Este caminho deve ser absoluto.
dags_folder = {AIRFLOW_HOME}/dags

# Hostname fornecendo um caminho para um callable, que resolverá o hostname.
# O formato é "package.function".
#
# Por exemplo, o valor padrão "airflow.utils.net.getfqdn" significa que o resultado do patch
# versão de socket.getfqdn() - veja https://github.com/python/cpython/issues/49254.
#
# Nenhum argumento deve ser necessário na função especificada.
# Se preferir usar o endereço IP como nome do host, use o valor ``airflow.utils.net.get_host_ip_address``
hostname_callable = airflow.utils.net.getfqdn

# Fuso horário padrão caso as datas fornecidas sejam ingênuas
# pode ser utc (padrão), sistema ou qualquer string de fuso horário IANA (por exemplo, Europa/Amsterdã)
default_timezone = utc

# A classe do executor que o fluxo de ar deve usar. As opções incluem
# ``SequentialExecutor``, ``LocalExecutor``, ``CeleryExecutor``, ``DaskExecutor``,
# ``KubernetesExecutor``, ``CeleryKubernetesExecutor`` ou o
# caminho de importação completo para a classe ao usar um executor personalizado.
executor = SequentialExecutor

# Isso define o número máximo de instâncias de tarefas que podem ser executadas simultaneamente por agendador em
# Fluxo de ar, independentemente da contagem de trabalhadores. Geralmente esse valor, multiplicado pelo número de
# agendadores em seu cluster, é o número máximo de instâncias de tarefas com a execução
# estado no banco de dados de metadados.
paralelismo = 32

# O número máximo de instâncias de tarefas que podem ser executadas simultaneamente em cada DAG. Calcular
# o número de tarefas que estão sendo executadas simultaneamente para um DAG, some o número de tarefas em execução
# tarefas para todas as execuções de DAG do DAG. Isso é configurável no nível do DAG com ``max_active_tasks``,
# que é padronizado como ``max_active_tasks_per_dag``.
#
# Um exemplo de cenário em que isso seria útil é quando você deseja parar um novo dag com um
# data de início do roubo de todos os slots do executor em um cluster.
max_active_tasks_per_dag = 16

# Os DAGs são pausados ​​por padrão na criação
dags_are_paused_at_creation = Verdadeiro

# O número máximo de execuções de DAG ativos por DAG. O agendador não criará mais execuções de DAG
# se atingir o limite. Isso é configurável no nível do DAG com ``max_active_runs``,
# que é padronizado como ``max_active_runs_per_dag``.
max_active_runs_per_dag = 16

# Se os exemplos de DAG fornecidos com o Airflow devem ser carregados. É bom para
# comece, mas você provavelmente deseja definir isso como ``False`` em uma produção
#ambiente _
load_examples = Verdadeiro

# Caminho para a pasta que contém os plugins do Airflow
plugins_folder = {AIRFLOW_HOME}/plugins

# As tarefas devem ser executadas via bifurcação do processo pai ("False",
# a opção mais rápida) ou gerando um novo processo python ("True" lento,
# mas significa que as alterações do plugin são captadas pelas tarefas imediatamente)
execute_tasks_new_python_interpreter = False

# Chave secreta para salvar senhas de conexão no banco de dados
fernet_key = {FERNET_KEY}

# Se deve desabilitar dags de decapagem
donot_pickle = Verdadeiro

# Quanto tempo antes do tempo limite de uma importação de arquivo python
dagbag_import_timeout = 30.0

# Caso um traceback seja mostrado na interface do usuário para erros de importação de dagbag,
# em vez de apenas a mensagem de exceção
dagbag_import_error_tracebacks = Verdadeiro

# Se tracebacks são mostrados, quantas entradas do traceback devem ser mostradas
dagbag_import_error_traceback_depth = 2

# Quanto tempo antes do tempo limite de um DagFileProcessor, que processa um arquivo dag
dag_file_processor_timeout = 50

# A classe a ser usada para executar instâncias de tarefa em um subprocesso.
# As opções incluem StandardTaskRunner, CgroupTaskRunner ou o caminho de importação completo para a classe
# ao usar um executor de tarefas personalizado.
task_runner = StandardTaskRunner

# Se definido, tarefas sem um argumento ``run_as_user`` serão executadas com este usuário
# Pode ser usado para reduzir a elevação de um usuário sudo executando o Airflow ao executar tarefas
default_impersonation =

# Qual módulo de segurança usar (por exemplo kerberos)
segurança =

# Ative o modo de teste de unidade (sobrescreve muitas opções de configuração com teste
# valores em tempo de execução)
unit_test_mode = False

# Se deve habilitar decapagem para xcom (observe que isso é inseguro e permite
# exploits RCE).
enable_xcom_pickling = Falso

# Quando uma tarefa é encerrada com força, esta é a quantidade de tempo em segundos que
# tem que limpar depois de ser enviado um SIGTERM, antes de ser SIGKILLED
kill_task_cleanup_time = 60

# Se os parâmetros devem ser substituídos com dag_run.conf. Se você passar alguns pares de valores-chave
# através de ``airflow dags backfill -c`` ou
# ``airflow dags trigger -c``, os pares chave-valor irão substituir os existentes em params.
dag_run_conf_overrides_params = Verdadeiro

# Ao descobrir DAGs, ignore quaisquer arquivos que não contenham as strings ``DAG`` e ``airflow``.
dag_discovery_safe_mode = Verdadeiro

# A sintaxe padrão usada nos arquivos ".airflowignore" nos diretórios do DAG. Os valores válidos são
# ``regexp`` ou ``glob``.
dag_ignore_file_syntax = regexp

# O número de tentativas que cada tarefa terá por padrão. Pode ser substituído no nível dag ou tarefa.
default_task_retries = 0

# O número de segundos que cada tarefa vai esperar por padrão entre as tentativas. Pode ser substituído em
# dag ou nível de tarefa.
default_task_retry_delay = 300

# O método de ponderação usado para o peso de prioridade total efetivo da tarefa
default_task_weight_rule = downstream

# O valor de execução_tempo limite da tarefa padrão para os operadores. Esperado um valor inteiro para
# ser passado para timedelta como segundos. Se não for especificado, o valor será considerado Nenhum,
# significando que os operadores nunca expiram por padrão.
default_task_execution_timeout =

# A atualização do DAG serializado não pode ser mais rápida que um intervalo mínimo para reduzir a taxa de gravação do banco de dados.
min_serialized_dag_update_interval = 30

# Se True, os DAGs serializados são compactados antes de gravar no banco de dados.
# Nota: isso desabilitará a visualização de dependências do DAG
compress_serialized_dags = False

# Buscar DAG serializado não pode ser mais rápido que um intervalo mínimo para reduzir o banco de dados
# taxa de leitura. Esta configuração controla quando seus DAGs são atualizados no servidor Web
min_serialized_dag_fetch_interval = 10

# Número máximo de campos de instância de tarefa renderizada (campos de modelo) por tarefa para armazenar
# no banco de dados.
# Todos os template_fields para cada instância de tarefa são armazenados no banco de dados.
# Manter este número pequeno pode causar um erro ao tentar visualizar a aba ``Rendered`` no
# Visualização TaskInstance para tarefas mais antigas.
max_num_rendered_ti_fields_per_task = 30

# Em cada dagrun, verifique os SLAs definidos
check_slas = Verdadeiro

# Caminho para a classe XCom personalizada que será usada para armazenar e resolver os resultados dos operadores
# Exemplo: xcom_backend = path.to.CustomXCom
xcom_backend = airflow.models.xcom.BaseXCom

# Por padrão, os plugins do Airflow são carregados lentamente (carregados apenas quando necessário). Defina como ``Falso``,
# se você quiser carregar plugins sempre que 'airflow' for invocado via cli ou carregado do módulo.
lazy_load_plugins = Verdadeiro

# Por padrão, os provedores do Airflow são descobertos lentamente (a descoberta e as importações ocorrem apenas quando necessário).
# Defina como False, se você quiser descobrir provedores sempre que 'airflow' for invocado via cli ou
# carregado do módulo.
lazy_discover_providers = Verdadeiro

# Ocultar variáveis ​​​​sensíveis ou chaves json extras de conexão da interface do usuário e dos logs de tarefas quando definido como True
#
# (As senhas de conexão estão sempre ocultas nos logs)
hide_sensitive_var_conn_fields = Verdadeiro

# Uma lista separada por vírgulas de palavras-chave extra sensíveis para procurar em nomes de variáveis ​​ou conexões
#JSON extra.
sensitive_var_conn_names =

# Task Slot conta para ``default_pool``. Esta configuração não teria nenhum efeito em um
# implantação onde o ``default_pool`` já foi criado. Para implantações existentes, os usuários podem
# altera o número de slots usando Webserver, API ou CLI
default_pool_task_slot_count = 128

# O comprimento máximo de lista/dict que um XCom pode enviar para acionar o mapeamento de tarefas. Se a lista/dict push tiver um
# comprimento excedendo este valor, a tarefa que empurra o XCom falhará automaticamente para evitar o
# tarefas mapeadas de entupimento do agendador.
max_map_length = 1024

# O umask padrão a ser usado para o processo quando executado no modo daemon (agendador, trabalhador, etc.)
#
# Isso controla a máscara do modo de criação de arquivo que determina o valor inicial dos bits de permissão de arquivo
# para arquivos recém-criados.
#
# Este valor é tratado como um inteiro octal.
daemon_umask = 0o077

# Classe a ser usada como gerenciador de eventos do dataset.
# Exemplo: dataset_event_manager_class = airflow.datasets.manager.DatasetEventManager
# dataset_event_manager_class =

# Kwargs para fornecer ao gerenciador de eventos do conjunto de dados.
# Exemplo: dataset_event_manager_kwargs = {{"some_param": "some_value"}}
# dataset_event_manager_kwargs =

[base de dados]
# A cadeia de conexão SqlAlchemy para o banco de dados de metadados.
# SqlAlchemy suporta muitos mecanismos de banco de dados diferentes.
# Mais informações aqui:
# http://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#database-uri
sql_alchemy_conn = sqlite:///{AIRFLOW_HOME}/airflow.db

# Argumentos de palavras-chave específicos do mecanismo extras passados ​​para o create_engine do SQLAlchemy, como um valor codificado em JSON
# Exemplo: sql_alchemy_engine_args = {{"arg1": True}}
# sql_alchemy_engine_args =

# A codificação para os bancos de dados
sql_engine_encoding = utf-8

# Agrupamento para colunas ``dag_id``, ``task_id``, ``key``, ``external_executor_id``
# caso tenham codificação diferente.
# Por padrão este agrupamento é o mesmo que o agrupamento do banco de dados, porém para ``mysql`` e ``mariadb``
# o padrão é ``utf8mb3_bin`` para que os tamanhos de índice de nossas chaves de índice não excedam
# o tamanho máximo do índice permitido quando o agrupamento é definido para a variante ``utf8mb4``
# (consulte https://github.com/apache/airflow/pull/17603#issuecomment-901121618).
# sql_engine_collation_for_ids =

# Se SqlAlchemy deve agrupar conexões de banco de dados.
sql_alchemy_pool_enabled = Verdadeiro

# O tamanho do pool SqlAlchemy é o número máximo de conexões de banco de dados
# na piscina. 0 indica nenhum limite.
sql_alchemy_pool_size = 5

# O tamanho máximo de estouro do pool.
# Quando o número de conexões com check-out atinge o tamanho definido em pool_size,
# conexões adicionais serão devolvidas até este limite.
# Quando essas conexões adicionais são devolvidas ao pool, elas são desconectadas e descartadas.
# Segue-se então que o número total de conexões simultâneas que o pool permitirá
# é pool_size + max_overflow,
# e o número total de conexões "adormecidas" que o pool permitirá é pool_size.
# max_overflow pode ser definido como ``-1`` para indicar que não há limite de overflow;
# nenhum limite será colocado no número total de conexões simultâneas. O padrão é ``10``.
sql_alchemy_max_overflow = 10

# A reciclagem do pool SqlAlchemy é o número de segundos que uma conexão
# pode ficar ocioso no pool antes de ser invalidado. Esta configuração faz
# não se aplica ao sqlite. Se o número de conexões de banco de dados for excedido,
# um valor de configuração mais baixo permitirá que o sistema se recupere mais rapidamente.
sql_alchemy_pool_recycle = 1800

# Verifique a conexão no início de cada check-out do pool de conexões.
# Normalmente, esta é uma instrução simples como "SELECT 1".
# Mais informações aqui:
# https://docs.sqlalchemy.org/en/14/core/pooling.html#disconnect-handling-pessimistic
sql_alchemy_pool_pre_ping = Verdadeiro

# O esquema a ser usado para o banco de dados de metadados.
# SqlAlchemy suporta bancos de dados com o conceito de múltiplos esquemas.
sql_alchemy_schema =

# Caminho de importação para argumentos de conexão em SqlAlchemy. O padrão é um dict vazio.
# Isso é útil quando você deseja configurar argumentos do mecanismo de banco de dados que o SqlAlchemy não analisará
# na cadeia de conexão.
# Consulte https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.connect_args
# sql_alchemy_connect_args =

# Se as conexões padrão fornecidas com o Airflow devem ser carregadas. É bom para
# comece, mas você provavelmente deseja definir isso como ``False`` em uma produção
#ambiente _
load_default_connections = Verdadeiro

# Número de vezes que o código deve ser repetido em caso de erros operacionais do banco de dados.
# Nem todas as transações serão repetidas, pois podem causar um estado indesejado.
# Atualmente só é usado em ``DagFileProcessor.process_file`` para tentar novamente ``dagbag.sync_to_db``.
max_db_retries = 3

[exploração madeireira]
# A pasta onde o airflow deve armazenar seus arquivos de log.
# Este caminho deve ser absoluto.
# Existem algumas configurações existentes que assumem que isso está definido como padrão.
# Se você optar por substituir isso, talvez seja necessário atualizar o dag_processor_manager_log_location e
# configurações dag_processor_manager_log_location também.
base_log_folder = {AIRFLOW_HOME}/logs

# O Airflow pode armazenar logs remotamente no AWS S3, Google Cloud Storage ou Elastic Search.
# Defina como True se desejar habilitar o log remoto.
remote_logging = False

# Os usuários devem fornecer um ID de conexão do Airflow que forneça acesso ao armazenamento
# localização. Dependendo do seu serviço de registro remoto, isso só pode ser usado para
# lendo logs, não escrevendo.
remote_log_conn_id =

# Caminho para o arquivo JSON da credencial do Google. Se omitido, a autorização com base no `padrão do aplicativo
# Credenciais
# <https://cloud.google.com/docs/authentication/production#finding_credentials_automatically>`__
# ser usado.
google_key_path =

# URL do bucket de armazenamento para registro remoto
# Os buckets do S3 devem começar com "s3://"
# Os grupos de log do Cloudwatch devem começar com "cloudwatch://"
# buckets do GCS devem começar com "gs://"
# Os buckets WASB devem começar com "wasb" apenas para ajudar o Airflow a selecionar o manipulador correto
# Os logs do Stackdriver devem começar com "stackdriver://"
remote_base_log_folder =

# Use a criptografia do lado do servidor para logs armazenados no S3
encrypt_s3_logs = Falso

# Nível de registro.
#
# Valores suportados: ``CRITICAL``, ``ERROR``, ``WARNING``, ``INFO``, ``DEBUG``.
logging_level = INFO

# Nível de registro para aipo. Se não estiver definido, ele usa o valor de logging_level
#
# Valores suportados: ``CRITICAL``, ``ERROR``, ``WARNING``, ``INFO``, ``DEBUG``.
celery_logging_level =

# Nível de log para a interface do usuário do Flask-appbuilder.
#
# Valores suportados: ``CRITICAL``, ``ERROR``, ``WARNING``, ``INFO``, ``DEBUG``.
fab_logging_level = AVISO

# Classe de registro
# Especifique a classe que especificará a configuração de log
# Esta classe deve estar no classpath do python
# Exemplo: logging_config_class = my.path.default_local_settings.LOGGING_CONFIG
logging_config_class =

# Sinalizar para habilitar/desabilitar logs coloridos no console
# Colorir os logs quando o terminal de controle for um TTY.
colour_console_log = Verdadeiro

# Formato de log para quando logs coloridos estão habilitados
colour_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {{%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} } %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colour_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

# Formato da linha de Log
log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s
log_formatter_class = airflow.utils.log.timezone_aware.TimezoneAware

# Especifique o padrão de prefixo como mencionado abaixo com o manipulador de fluxo TaskHandlerWithCustomFormatter
# Exemplo: task_log_prefix_template = {{ti.dag_id}}-{{ti.task_id}}-{{execution_date}}-{{try_number}}
task_log_prefix_template =

# Formatação de como o airflow gera nomes/caminhos de arquivo para cada execução de tarefa.
log_filename_template = dag_id ={{{{ ti.dag_id }}}}/ run_id ={{{{ ti.run_id }}}}/ task_id ={{{{ ti.task_id }}}}/{{%% if ti .map_index >= 0 %%}} map_index ={{{{ ti.map_index }}}}/{{%% endif %%}} tentativa ={{{{ try_number }}}}.log

# Formatação de como o airflow gera nomes de arquivos para log
log_processor_filename_template = {{{{ filename }}}}.log

# Caminho completo do arquivo de log dag_processor_manager.
dag_processor_manager_log_location = {AIRFLOW_HOME}/logs/dag_processor_manager/dag_processor_manager.log

# Nome do manipulador para ler os logs da instância da tarefa.
# Padrões para usar o manipulador ``task``.
task_log_reader = tarefa

# Uma lista separada por vírgulas de nomes de registradores de terceiros que serão configurados para imprimir mensagens para
#console \.
# Exemplo: extra_logger_names = conexão,sqlalchemy
extra_logger_names =

# Quando você inicia um trabalhador de fluxo de ar, o fluxo de ar inicia um pequeno servidor web
# subprocesso para servir os arquivos de log locais dos trabalhadores para o fluxo de ar principal
# servidor web, que então constrói as páginas e as envia aos usuários. Isso define
# a porta na qual os logs são servidos. Ele precisa ser usado e aberto
# visível do servidor web principal para se conectar aos trabalhadores.
worker_log_server_port = 8793

[Métricas]

# StatsD (https://github.com/etsy/statsd) configurações de integração.
# Habilita o envio de métricas para o StatsD.
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = fluxo de ar

# Se você quiser evitar o envio de todas as métricas disponíveis para o StatsD,
# você pode configurar uma lista de prefixos (separados por vírgulas) para enviar apenas as métricas que
# inicia com os elementos da lista (ex: "scheduler,executor,dagrun")
statsd_allow_list =

# Uma função que valida o nome da estatística StatsD, aplica alterações no nome da estatística se necessário e retorna
# o nome da estatística transformada.
#
# A função deve ter a seguinte assinatura:
# def func_name(stat_name: str) -> str:
stat_name_handler =

# Para habilitar a integração de datadog para enviar métricas de fluxo de ar.
statsd_datadog_enabled = False

# Lista de tags datadog anexadas a todas as métricas (por exemplo: key1:value1,key2:value2)
statsd_datadog_tags =

# Se você quiser utilizar seu próprio cliente StatsD personalizado, defina o
# caminho do módulo abaixo.
# Nota: O caminho do módulo deve existir em seu PYTHONPATH para que o Airflow o pegue
# statsd_custom_client_path =

[segredos]
# Nome completo da classe do backend de segredos para habilitar (precederá env vars e metastore no caminho de pesquisa)
# Exemplo: backend = airflow.providers.amazon.aws.secrets.systems_manager.SystemsManagerParameterStoreBackend
back-end =

# O parâmetro backend_kwargs é carregado em um dicionário e passado para a classe de backend __init__ de segredos.
# Consulte a documentação do backend de segredos que você está usando. JSON é esperado.
# Exemplo para o AWS Systems Manager ParameterStore:
# ``{{"connections_prefix": "/airflow/connections", "profile_name": "default"}}``
backend_kwargs =

[cli]
# De que forma o cli deve acessar a API. O LocalClient usará o
# banco de dados diretamente, enquanto o json_client usará a api em execução no
# servidor web
api_client = airflow.api.client.local_client

# Se você definir web_server_url_prefix, NÃO se esqueça de anexá-lo aqui, ex:
# ``endpoint_url = http://localhost:8080/myroot``
# Então api ficará assim: ``http://localhost:8080/myroot/api/experimental/...``
endpoint_url = http://localhost:8080

[depurar]
# Usado apenas com ``DebugExecutor``. Se definido como ``True``, o DAG falhará com o primeiro
# tarefa falhou. Útil para fins de depuração.
fail_fast = False

[api]
# Ativa a API experimental obsoleta. Observe que essas APIs não têm controle de acesso.
# O usuário autenticado tem acesso total.
#
# .. aviso::
#
#    Esta `API REST experimental <https://airflow.readthedocs.io/en/latest/rest-api-ref.html>`__ é
#    obsoleto desde a versão 2.0. Por favor, considere usar
#    `a API REST estável <https://airflow.readthedocs.io/en/latest/stable-rest-api-ref.html>`__.
#    Para obter mais informações sobre migração, consulte
#    `RELEASE_NOTES.rst <https://github.com/apache/airflow/blob/main/RELEASE_NOTES.rst>`_
enable_experimental_api = False

# Lista separada por vírgulas de back-ends de autenticação para autenticar usuários da API. Ver
# https://airflow.apache.org/docs/apache-airflow/stable/security/api.html para valores possíveis.
# ("airflow.api.auth.backend.default" permite todas as solicitações por motivos históricos)
auth_backends = airflow.api.auth.backend.session

# Usado para definir o limite máximo de páginas para solicitações de API
maximum_page_limit = 100

# Usado para definir o limite de página padrão quando o limite é zero. Um limite padrão
# de 100 está definido na especificação OpenApi. No entanto, esse limite padrão específico
# só funciona quando o limite é igual a zero(0) das solicitações da API.
# Se nenhum limite for fornecido, o padrão de especificação OpenApi será usado.
fallback_page_limit = 100

# O público pretendido para credenciais de token JWT usadas para autorização. Esse valor deve corresponder nos lados do cliente e do servidor. Se estiver vazio, o público não será testado.
# Exemplo: google_oauth2_audience = project-id-random-value.apps.googleusercontent.com
google_oauth2_audience =

# Caminho para o arquivo de chave da conta de serviço do Google Cloud (JSON). Se omitido, a autorização baseada em
# `as credenciais padrão do aplicativo
# <https://cloud.google.com/docs/authentication/production#finding_credentials_automatically>`__
# ser usado.
# Exemplo: google_key_path = /files/service-account-json
google_key_path =

# Usado em resposta a uma solicitação de comprovação para indicar qual HTTP
# cabeçalhos podem ser usados ​​ao fazer a solicitação real. Este cabeçalho é
# a resposta do lado do servidor para o navegador
# Cabeçalho Access-Control-Request-Headers.
access_control_allow_headers =

# Especifica o método ou métodos permitidos ao acessar o recurso.
access_control_allow_methods =

# Indica se a resposta pode ser compartilhada com o código solicitante das origens fornecidas.
# Separe os URLs com espaço.
access_control_allow_origins =

[linhagem]
# qual back-end de linhagem usar
back-end =

[Atlas]
sasl_enabled = False
anfitrião =
porta = 21000
nome de usuário =
senha =

[operadores]
# O proprietário padrão atribuído a cada novo operador, a menos que
# fornecido explicitamente ou passado via ``default_args``
proprietário_padrão = fluxo de ar
default_cpus = 1
default_ram = 512
disco_padrão = 512
default_gpus = 0

# Fila padrão à qual as tarefas são atribuídas e que o trabalhador escuta.
default_queue = default

# Tem permissão para passar argumentos adicionais/não usados ​​(args, kwargs) para o operador BaseOperator.
# Se definido como False, uma exceção será lançada, caso contrário, apenas a mensagem do console será exibida.
allow_illegal_arguments = False

[colmeia]
# Fila mapreduce padrão para tarefas HiveOperator
default_hive_mapred_queue =

# Modelo para mapred_job_name no HiveOperator, suporta os seguintes parâmetros nomeados
# hostname, dag_id, task_id, execução_date
# mapred_job_name_template =

[servidor web]
# O URL base do seu site como fluxo de ar não pode adivinhar qual domínio ou
# cname que você está usando. Isso é usado em e-mails automatizados que
# airflow envia links de ponto para o servidor web correto
base_url = http://localhost:8080

# Fuso horário padrão para exibir todas as datas na interface do usuário, pode ser UTC, sistema ou
# qualquer string de fuso horário IANA (por exemplo, Europa/Amsterdã). Se deixado vazio o
# valor padrão de core/default_timezone será usado
# Exemplo: default_ui_timezone = America/New_York
default_ui_timezone = UTC

# O ip especificado ao iniciar o servidor web
web_server_host = 0.0.0.0

# A porta na qual executar o servidor web
web_server_port = 8080

# Caminhos para o certificado SSL e chave para o servidor web. Quando ambos são
# desde que o SSL seja ativado. Isso não altera a porta do servidor web.
web_server_ssl_cert =

# Caminhos para o certificado SSL e chave para o servidor web. Quando ambos são
# desde que o SSL seja ativado. Isso não altera a porta do servidor web.
web_server_ssl_key =

# O tipo de backend usado para armazenar os dados da sessão da web, pode ser 'banco de dados' ou 'securecookie'
# Exemplo: session_backend = securecookie
session_backend = banco de dados

# Número de segundos que o servidor web espera antes de matar o mestre do gunicorn que não responde
web_server_master_timeout = 120

# Número de segundos que o servidor web gunicorn espera antes de expirar em um trabalhador
web_server_worker_timeout = 120

# Número de trabalhadores a serem atualizados por vez. Quando definido como 0, a atualização do trabalhador é
# desabilitado. Quando diferente de zero, o fluxo de ar atualiza periodicamente os trabalhadores do servidor web
# criando novos e matando os antigos.
trabalhador_refresh_batch_size = 1

# Número de segundos a esperar antes de atualizar um lote de workers.
trabalhador_refresh_interval = 6000

# Se definido como True, o Airflow rastreará os arquivos no diretório plugins_folder. Ao detectar alterações,
# então recarregue o gunicorn.
reload_on_plugin_change = False

# Chave secreta usada para executar seu aplicativo de frasco. Deve ser o mais aleatório possível. No entanto, ao executar
# mais de 1 instâncias de servidor web, certifique-se de que todas elas usem a mesma ``secret_key`` caso contrário
# um deles apresentará erro com "falta o token de sessão CSRF".
# A chave do servidor da web também é usada para autorizar solicitações aos trabalhadores do Celery quando os logs são recuperados.
# O token gerado usando a chave secreta tem um tempo de expiração curto - certifique-se de que esse tempo esteja ativado
# TODAS as máquinas nas quais você executa os componentes do fluxo de ar são sincronizadas (por exemplo, usando ntpd)
# caso contrário, você poderá obter erros "proibidos" quando os logs forem acessados.
chave_secreta = {SECRET_KEY}

# Número de trabalhadores para executar o servidor web Gunicorn
trabalhadores = 4

# O gunicorn da classe trabalhadora deve usar. As opções incluem
# sync (padrão), eventlet, gevent
classe_trabalhadora = sincronizar

# Arquivos de log para o servidor web gunicorn. '-' significa log para stderr.
access_logfile = -

# Arquivos de log para o servidor web gunicorn. '-' significa log para stderr.
error_logfile = -

# Formato de log de acesso para o servidor web gunicorn.
# formato padrão é %%(h)s %%(l)s %%(u)s %%(t)s "%%(r)s" %%(s)s %%(b)s "% %(f)s" "%%(a)s"
# documentação - https://docs.gunicorn.org/en/stable/settings.html#access-log-format
access_logformat =

# Expor o arquivo de configuração no servidor web
expor_config = False

# Expor hostname no servidor web
expor_hostname = Verdadeiro

# Expor stacktrace no servidor web
expor_stacktrace = Verdadeiro

# Visualização padrão do DAG. Os valores válidos são: ``grid``, ``graph``, ``duration``, ``gantt``, ``landing_times``
dag_default_view = grade

# Orientação padrão do DAG. Os valores válidos são:
# ``LR`` (Esquerda->Direita), ``TB`` (Superior->Inferior), ``RL`` (Direita->Esquerda), ``BT`` (Inferior->Superior)
dag_orientation = LR

# A quantidade de tempo (em segundos) que o servidor web aguardará pelo handshake inicial
# ao buscar logs de outra máquina de trabalho
log_fetch_timeout_sec = 5

# Intervalo de tempo (em segundos) para esperar antes da próxima busca de log.
log_fetch_delay_sec = 2

# Distância da parte inferior da página para habilitar a cauda automática.
log_auto_tailing_offset = 30

# Velocidade de animação para exibição de log de cauda automática.
log_animation_speed = 1000

# Por padrão, o servidor web mostra DAGs pausados. Vire isso para ocultar pausado
# DAGs por padrão
hide_paused_dags_by_default = False

# Tamanho de página consistente em todas as visualizações de listagem na interface do usuário
page_size = 100

# Define a cor da barra de navegação
navbar_color = #fff _

# Dagrun padrão para mostrar na interface do usuário
default_dag_run_display_number = 25

# Habilita o middleware werkzeug ``ProxyFix`` para proxy reverso
enable_proxy_fix = False

# Número de valores a serem confiáveis ​​para ``X-Forwarded-For``.
# Mais informações: https://werkzeug.palletsprojects.com/en/0.16.x/middleware/proxy_fix/
proxy_fix_x_for = 1

# Número de valores a serem confiáveis ​​para ``X-Forwarded-Proto``
proxy_fix_x_proto = 1

# Número de valores a serem confiáveis ​​para ``X-Forwarded-Host``
proxy_fix_x_host = 1

# Número de valores a serem confiáveis ​​para ``X-Forwarded-Port``
proxy_fix_x_port = 1

# Número de valores a serem confiáveis ​​para ``X-Forwarded-Prefix``
proxy_fix_x_prefix = 1

# Definir sinalizador seguro no cookie de sessão
cookie_secure = False

# Definir política do mesmo site no cookie de sessão
cookie_samesite = Lax

# Configuração padrão para alternância de wrap no código DAG e visualizações de log de TI.
default_wrap = False

# Permite que a interface do usuário seja renderizada em um frame
x_frame_enabled = Verdadeiro

# Envie atividades de usuários anônimos para sua ferramenta de análise
# escolha entre google_analytics, segmento ou metaroteador
# analytics_tool =

# ID exclusivo da sua conta na ferramenta de análise
# analytics_id =

# As estatísticas de 'Tarefas recentes' serão exibidas para DagRuns antigas, se definidas
show_recent_stats_for_completed_runs = Verdadeiro

# Atualize as permissões do FAB e sincronize as funções do gerenciador de segurança
# na inicialização do servidor web
update_fab_perms = Verdadeiro

# O tempo de vida do cookie da interface do usuário em minutos. O usuário será desconectado da interface do usuário após
# ``session_lifetime_minutes`` de não atividade
session_lifetime_minutes = 43200

# Define um título de página personalizado para a página de visão geral dos DAGs e o título do site para todas as páginas
# instance_name =

# Se o título da página personalizada para a página de visão geral dos DAGs contém qualquer linguagem de marcação
instance_name_has_markup = False

# Com que frequência, em segundos, os dados do DAG serão atualizados automaticamente no gráfico ou na exibição de grade
# quando a atualização automática está ativada
auto_refresh_interval = 3

# Boolean para exibir aviso para implantação publicamente visível
warning_deployment_exposure = Verdadeiro

# Sequência de eventos de exibição separada por vírgula para excluir da exibição de auditoria dag.
# Todos os outros eventos serão adicionados menos os passados ​​aqui.
# Os logs de auditoria no banco de dados não serão afetados por este parâmetro.
audit_view_excluded_events = gantt,landing_times,tentativas,duração,calendário,gráfico,grade,árvore,árvore_dados

# Sequência de eventos de exibição separada por vírgulas para incluir na exibição de auditoria dag.
# Se aprovado, apenas esses eventos preencherão a exibição de auditoria dag.
# Os logs de auditoria no banco de dados não serão afetados por este parâmetro.
# Exemplo: audit_view_included_events = dagrun_cleared,failed
# audit_view_included_events =

[o email]

# Backend de e-mail de configuração e se deve
# enviar alertas por e-mail em uma nova tentativa ou falha
# Backend de e-mail a ser usado
email_backend = airflow.utils.email.send_email_smtp

# Conexão de e-mail a ser usada
email_conn_id = smtp_default

# Se os alertas de e-mail devem ser enviados quando uma tarefa é repetida
default_email_on_retry = Verdadeiro

# Se os alertas de e-mail devem ser enviados quando uma tarefa falhou
default_email_on_failure = Verdadeiro

# Arquivo que será usado como template para assunto do Email (que será renderizado usando Jinja2).
# Se não estiver definido, o Airflow usa um modelo básico.
# Exemplo: subject_template = /path/to/my_subject_template_file
# assunto_modelo =

# Arquivo que será usado como template para o conteúdo do Email (que será renderizado usando Jinja2).
# Se não estiver definido, o Airflow usa um modelo básico.
# Exemplo: html_content_template = /path/to/my_html_content_template_file
# html_content_template =

# Endereço de e-mail que será usado como endereço do remetente.
# Pode ser um email bruto ou o endereço completo no formato ``Sender Name <sender@email.com>``
# Exemplo: from_email = Airflow <airflow@example.com>
# from_email =

[smtp]

# Se você deseja que o fluxo de ar envie e-mails em tentativas, falhas e deseja usar
# a função airflow.utils.email.send_email_smtp, você deve configurar um
# servidor smtp aqui
smtp_host = localhost
smtp_starttls = Verdadeiro
smtp_ssl = Falso
# Exemplo: smtp_user = fluxo de ar
# smtp_user =
# Exemplo: smtp_password = airflow
# smtp_password =
smtp_port = 25
smtp_mail_from = airflow@example.com
smtp_timeout = 30
smtp_retry_limit = 5

[sentinela]

# Integração do Sentry (https://docs.sentry.io). Aqui você pode fornecer
# opções de configuração adicionais baseadas na plataforma Python. Ver:
# https://docs.sentry.io/error-reporting/configuration/?platform=python.
# Opções não suportadas: ``integrations``, ``in_app_include``, ``in_app_exclude``,
# ``ignore_errors``, ``before_breadcrumb``, ``transport``.
# Habilite o relatório de erros para o Sentinela
sentry_on = false
sentry_dsn =

# Caminho pontilhado para uma função before_send que o sentry SDK deve ser configurado para usar.
#antes_enviar =

[local_kubernetes_executor]

# Esta seção só se aplica se você estiver usando o ``LocalKubernetesExecutor`` em
# seção ``[core]`` acima
# Defina quando enviar uma tarefa para o ``KubernetesExecutor`` ao usar o ``LocalKubernetesExecutor``.
# Quando a fila de uma tarefa é o valor de ``kubernetes_queue`` (padrão ``kubernetes``),
# a tarefa é executada via ``KubernetesExecutor``,
# caso contrário via ``LocalExecutor``
kubernetes_queue = kubernetes

[celery_kubernetes_executor]

# Esta seção só se aplica se você estiver usando o ``CeleryKubernetesExecutor`` em
# seção ``[core]`` acima
# Defina quando enviar uma tarefa para o ``KubernetesExecutor`` ao usar o ``CeleryKubernetesExecutor``.
# Quando a fila de uma tarefa é o valor de ``kubernetes_queue`` (padrão ``kubernetes``),
# a tarefa é executada via ``KubernetesExecutor``,
# caso contrário via ``CeleryExecutor``
kubernetes_queue = kubernetes

[salsão]

# Esta seção se aplica somente se você estiver usando o CeleryExecutor em
# seção ``[core]`` acima
# O nome do aplicativo que será usado pelo aipo
celery_app_name = airflow.executors.celery_executor

# A simultaneidade que será usada ao iniciar workers com o
# comando ``airflow aipo worker``. Isso define o número de instâncias de tarefa que
# um trabalhador levará, então dimensione seus trabalhadores com base nos recursos em
# sua caixa de trabalho e a natureza de suas tarefas
worker_concurrency = 16

# A simultaneidade máxima e mínima que será usada ao iniciar trabalhadores com o
# Comando ``airflow celery worker`` (sempre mantenha processos mínimos, mas aumente
# ao máximo, se necessário). Observe que o valor deve ser max_concurrency,min_concurrency
# Escolha esses números com base nos recursos da caixa do trabalhador e na natureza da tarefa.
# Se a opção de dimensionamento automático estiver disponível, worker_concurrency será ignorado.
# http://docs.celeryproject.org/en/latest/reference/celery.bin.worker.html#cmdoption-celery-worker-autoscale
# Exemplo: worker_autoscale = 16,12
# worker_autoscale =

# Usado para aumentar o número de tarefas que um trabalhador pré-busca, o que pode melhorar o desempenho.
# O número de processos multiplicado por worker_prefetch_multiplier é o número de tarefas
# que são pré-buscados por um trabalhador. Um valor maior que 1 pode resultar em tarefas sendo desnecessariamente
# bloqueado se houver vários trabalhadores e um trabalhador pré-busca tarefas que ficam para trás por muito tempo
# executando tarefas enquanto outro trabalhador tem processos não utilizados que não podem processar os já
# reivindicou tarefas bloqueadas.
# https://docs.celeryproject.org/en/stable/userguide/optimizing.html#prefetch-limits
worker_prefetch_multiplier = 1

# Especifique se o controle remoto dos trabalhadores está habilitado.
# Ao usar o Amazon SQS como agente, o Celery cria muitas filas ``.*reply-celery-pidbox``. Você pode
# evite isso configurando como false. No entanto, com isso desabilitado, o Flower não funcionará.
worker_enable_remote_control = true

# A URL do corretor de aipo. Aipo suporta RabbitMQ, Redis e experimentalmente
# um banco de dados sqlalchemy. Consulte a documentação do Aipo para obter mais informações.
broker_url = redis://redis:6379/0

# O result_backend do Aipo. Quando um trabalho termina, ele precisa atualizar o
# metadados do trabalho. Portanto, ele postará uma mensagem em um barramento de mensagens,
# ou insira-o em um banco de dados (dependendo do backend)
# Este status é usado pelo agendador para atualizar o estado da tarefa
# O uso de um banco de dados é altamente recomendado
# Quando não especificado, sql_alchemy_conn com um prefixo de esquema db+ será usado
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#task-result-backend-settings
# Exemplo: result_backend = db+postgresql://postgres:airflow@postgres/airflow
# result_backend =

# Flor de aipo é uma interface de usuário doce para aipo. O Airflow tem um atalho para iniciar
# it ``flor de aipo de fluxo de ar``. Isso define o IP em que o Celery Flower é executado
host_flor = 0.0.0.0

# A URL raiz para Flor
# Exemplo: flower_url_prefix = /flower
flower_url_prefix =

# Isso define a porta em que o Celery Flower é executado
porta_flor = 5555

# Protegendo a Flor com Autenticação Básica
# Aceita pares de usuário:senha separados por vírgula
# Exemplo: flower_basic_auth = user1:password1,user2:password2
flower_basic_auth =

# Quantos processos o CeleryExecutor usa para sincronizar o estado da tarefa.
# 0 significa usar max(1, número de núcleos - 1) processos.
sync_parallelism = 0

# Caminho de importação para opções de configuração de aipo
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG
ssl_active = False
ssl_key =
ssl_cert =
ssl_cacert =

# Implementação de pool de aipo.
# As opções incluem: ``prefork`` (padrão), ``eventlet``, ``gevent`` ou ``solo``.
# Veja:
# https://docs.celeryproject.org/en/latest/userguide/workers.html#concurrency
# https://docs.celeryproject.org/en/latest/userguide/concurrency/eventlet.html
piscina = prefork

# O número de segundos para esperar antes do tempo limite ``send_task_to_executor`` ou
# Operações ``fetch_celery_task_state``.
operação_tempo limite = 1,0

# A tarefa de aipo relatará seu status como 'iniciada' quando a tarefa for executada por um trabalhador.
# Isso é usado no Airflow para acompanhar as tarefas em execução e se um Agendador for reiniciado
# ou executado no modo HA, ele pode adotar as tarefas órfãs iniciadas pelo SchedulerJob anterior.
task_track_started = Verdadeiro

# Tempo em segundos após o qual as tarefas adotadas que são enfileiradas no aipo são consideradas paralisadas,
# e são reprogramados automaticamente. Esta configuração faz a mesma coisa que ``stalled_task_timeout`` mas
# aplica-se especificamente apenas às tarefas adotadas. Quando definido como 0, a configuração ``stalled_task_timeout``
# também se aplica às tarefas adotadas.
task_adoption_timeout = 600

# Tempo em segundos após o qual as tarefas enfileiradas no aipo são consideradas paralisadas e são automaticamente
# reagendado. As tarefas adotadas usarão a configuração ``task_adoption_timeout`` se especificada.
# Quando definido como 0, a limpeza automática de tarefas paralisadas é desabilitada.
stalled_task_timeout = 0

# O número máximo de tentativas para publicar mensagens de tarefa no broker ao falhar
# devido ao erro ``AirflowTaskTimeout`` antes de desistir e marcar a tarefa como com falha.
task_publish_max_retries = 3

# Verificação de inicialização do trabalhador para validar a conexão do banco de dados de metadados
worker_precheck = False

[celery_broker_transport_options]

# Esta seção é para especificar opções que podem ser passadas para o
# transporte de corretor de aipo subjacente. Ver:
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#std:setting-broker_transport_options
# O tempo limite de visibilidade define o número de segundos para esperar pelo trabalhador
# para reconhecer a tarefa antes que a mensagem seja reenviada para outro trabalhador.
# Certifique-se de aumentar o tempo limite de visibilidade para corresponder ao tempo do maior
# ETA que você planeja usar.
# visible_timeout é compatível apenas com corretores de aipo Redis e SQS.
# Veja:
# http://docs.celeryproject.org/en/master/userguide/configuration.html#std:setting-broker_transport_options
# Exemplo: visibilidade_tempo limite = 21600
# visibilidade_tempo limite =

[dask]

# Esta seção só se aplica se você estiver usando o DaskExecutor em
# seção [core] acima
# O endereço IP e a porta do agendador do cluster Dask.
cluster_address = 127.0.0.1:8786

# Configurações TLS/SSL para acessar um agendador Dask seguro.
tls_ca =
tls_cert =
tls_key =

[Agendador]
# Instâncias de tarefas escutam sinal de kill externo (quando você limpa tarefas
# da CLI ou da UI), isso define a frequência com que eles devem
# escuta (em segundos).
job_heartbeat_sec = 5

# O escalonador tenta constantemente acionar novas tarefas (veja o
# seção do agendador nos documentos para obter mais informações). Isso define
# com que frequência o agendador deve ser executado (em segundos).
scheduler_heartbeat_sec = 5

# O número de vezes para tentar agendar cada arquivo DAG
# -1 indica um número ilimitado
num_runs = -1

# Controla quanto tempo o escalonador irá dormir entre os loops, mas se não houver nada a fazer
# no laço. ou seja, se programou algo, ele iniciará o próximo loop
# iteração imediatamente.
scheduler_idle_sleep_time = 1

# Número de segundos após o qual um arquivo DAG é analisado. O arquivo DAG é analisado a cada
# ``min_file_process_interval`` número de segundos. As atualizações nos DAGs são refletidas após
# este intervalo. Manter esse número baixo aumentará o uso da CPU.
min_file_process_interval = 30

# Com que frequência (em segundos) verificar DAGs obsoletos (DAGs que não estão mais presentes no
# os arquivos esperados) que devem ser desativados.
deactivate_stale_dags_interval = 60

# Com que frequência (em segundos) verificar o diretório de DAGs em busca de novos arquivos. Padrão para 5 minutos.
dag_dir_list_interval = 300

# Com que frequência as estatísticas devem ser impressas nos logs. Definir como 0 desativará as estatísticas de impressão
print_stats_interval = 30

# Com que frequência (em segundos) as estatísticas de uso do pool devem ser enviadas para StatsD (se statsd_on estiver ativado)
pool_metrics_interval = 5,0

# Se a última pulsação do agendador ocorreu mais do que scheduler_health_check_threshold
# atrás (em segundos), o agendador é considerado não íntegro.
# Isso é usado pela verificação de integridade no endpoint "/health"
scheduler_health_check_threshold = 30

# Quando você inicia um agendador, o airflow inicia um pequeno servidor web
# subprocesso para servir uma verificação de integridade se estiver definido como True
enable_health_check = Falso

# Quando você inicia um agendador, o airflow inicia um pequeno servidor web
# subprocesso para servir uma verificação de integridade nesta porta
scheduler_health_check_server_port = 8974

# Com que frequência (em segundos) o agendador deve verificar tarefas órfãs e SchedulerJobs
orphaned_tasks_check_interval = 300,0
child_process_log_directory = {AIRFLOW_HOME}/logs/scheduler

# Trabalhos de tarefas locais pulsam periodicamente para o banco de dados. Se o trabalho tiver
# não pulsação em tantos segundos, o agendador marcará o
# instância de tarefa associada com falha e reagendará a tarefa.
scheduler_zombie_task_threshold = 300

# Com que frequência (em segundos) o agendador deve verificar as tarefas zumbis.
zombie_detection_interval = 10.0

# Desative o catchup do agendador configurando isso para ``False``.
# O comportamento padrão é inalterado e
# Os preenchimentos de linha de comando ainda funcionam, mas o agendador
# não fará o catchup do agendador se for ``False``,
# no entanto, pode ser definido por DAG no
# Definição de DAG (catchup)
catchup_by_default = Verdadeiro

# Definir isso como True fará a primeira instância de uma tarefa
# ignora a configuração Depends_on_past. Uma instância de tarefa será considerada
# como a primeira instância de tarefa de uma tarefa quando não há instância de tarefa
# no banco de dados com uma data_execução anterior a ela., ou seja, sem marcação manual
# sucesso será necessário para que uma tarefa recém-adicionada seja agendada.
ignore_first_depends_on_past_by_default = Verdadeiro

# Isso altera o tamanho do lote de consultas no loop principal de agendamento.
# Se for muito alto, o desempenho da consulta SQL pode ser afetado por
# complexidade do predicado da consulta e/ou bloqueio excessivo.
# Além disso, você pode atingir o comprimento máximo de consulta permitido para seu banco de dados.
# Defina isso para 0 sem limite (não recomendado)
max_tis_per_query = 512

# Se o agendador emitir ``SELECT ... FOR UPDATE`` em consultas relevantes.
# Se estiver definido como False, você não deve executar mais de um
# agendador de uma vez
use_row_level_locking = Verdadeiro

# Número máximo de DAGs para criar DagRuns por loop do agendador.
max_dagruns_to_create_per_loop = 10

# Quantos DagRuns um agendador deve examinar (e bloquear) ao agendar
# e tarefas de enfileiramento.
max_dagruns_per_loop_to_schedule = 20

# Caso o processo supervisor de tarefas execute um "mini agendador" para tentar agendar mais tarefas do
# mesmo DAG. Deixar isso ativado significará que as tarefas no mesmo DAG serão executadas mais rapidamente, mas poderá deixar outros
# dags em algumas circunstâncias
schedule_after_task_execution = Verdadeiro

# O escalonador pode executar vários processos em paralelo para analisar dags.
# Isso define quantos processos serão executados.
parsing_processes = 2

# Um de ``modified_time``, ``random_seeded_by_host`` e ``alphabetical``.
# O agendador listará e classificará os arquivos dag para decidir a ordem de análise.
#
# * ``modified_time``: Ordena por hora de modificação dos arquivos. Isso é útil em larga escala para analisar o
#    DAGs recentemente modificados primeiro.
# * ``random_seeded_by_host``: Classifica aleatoriamente em vários Schedulers, mas com a mesma ordem no
#    mesmo host. Isso é útil ao executar com o Agendador no modo HA, onde cada agendador pode
#    analisa diferentes arquivos DAG.
# * ``alfabética``: Ordenar por nome de arquivo
file_parsing_sort_mode = horário_modificado

# Se o processador dag está sendo executado como um processo autônomo ou é um subprocesso de um escalonador
#trabalho .
standalone_dag_processor = False

# Aplicável apenas se `[scheduler]standalone_dag_processor` for true e os callbacks forem armazenados
# no banco de dados. Contém o número máximo de retornos de chamada que são buscados durante um único loop.
max_callbacks_per_loop = 20

# Desative o uso do agendador de intervalos cron configurando-o como False.
# DAGs enviados manualmente na interface do usuário da Web ou com trigger_dag ainda serão executados.
use_job_schedule = Verdadeiro

# Permitir DagRuns acionados externamente para datas de execução no futuro
# Só tem efeito se schedule_interval estiver definido como Nenhum no DAG
allow_trigger_in_future = False

# Com que frequência verificar solicitações de gatilho expiradas que ainda não foram executadas.
trigger_timeout_check_interval = 15

[gatilho]
# Quantos triggers um único Triggerer irá executar de uma vez, por padrão.
default_capacity = 1000

[kerberos]
ccache = /tmp/airflow_krb5_ccache

# é aumentado com fqdn
principal = fluxo de ar
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab

# Permite desabilitar o encaminhamento do ticket.
encaminhável = Verdadeiro

# Permitir remover o IP de origem do token, útil ao usar o token por trás do host NATted Docker.
include_ip = Verdadeiro

[pesquisa elástica]
# Hospedagem Elasticsearch
anfitrião =

# Formato do log_id, que é usado para consultar os logs de determinadas tarefas
log_id_template = {{dag_id}}-{{task_id}}-{{run_id}}-{{map_index}}-{{try_number}}

# Usado para marcar o fim de um fluxo de log para uma tarefa
end_of_log_mark = end_of_log

# URL qualificado para um frontend elasticsearch (como Kibana) com um argumento de modelo para log_id
# O código construirá log_id usando o modelo log_id do argumento acima.
# OBSERVAÇÃO: o esquema será padrão para https se não for fornecido
# Exemplo: frontend = http://localhost:5601/app/kibana#/discover?_a=(columns:!(message),query:(language:kuery,query:'log_id: "{{log_id}}"' ),sort:!(log.offset,asc))
interface =

# Grava os logs de tarefas no stdout do trabalhador, em vez dos arquivos padrão
write_stdout = False

# Em vez do formatador de log padrão, escreva as linhas de log como JSON
json_format = False

# Campos de log para também anexar à saída json, se habilitado
json_fields = asctime, nome do arquivo, lineno, levelname, mensagem

# O campo onde o nome do host é armazenado (normalmente `host` ou `host.name`)
host_field = host

# O campo onde o deslocamento é armazenado (normalmente `offset` ou `log.offset`)
campo_deslocamento = deslocamento

[elasticsearch_configs]
use_ssl = False
verifique_certs = Verdadeiro

[kubernetes]
# Caminho para o arquivo de pod YAML que forma a base para os workers do KubernetesExecutor.
pod_template_file =

# O repositório da imagem do Kubernetes para o trabalhador executar
worker_container_repository =

# A tag da imagem do Kubernetes para o trabalhador executar
trabalhador_container_tag =

# O namespace do Kubernetes onde os trabalhadores do fluxo de ar devem ser criados. Padrões para ``padrão``
namespace = padrão

# Se True, todos os pods de trabalho serão excluídos após o término
delete_worker_pods = Verdadeiro

# Se False (e delete_worker_pods for True),
# pods de trabalho com falha não serão excluídos para que os usuários possam investigá-los.
# Isso só impede a remoção de pods de trabalhador onde o próprio trabalhador falhou,
# não quando a tarefa que executou falhou.
delete_worker_pods_on_failure = Falso

# Número de chamadas de criação do Kubernetes Worker Pod por loop do agendador.
# Observe que o padrão atual de "1" iniciará apenas um único pod
# por batimento cardíaco. É ALTAMENTE recomendado que os usuários aumentem este
# número para corresponder à tolerância do cluster kubernetes para
# melhor desempenho.
worker_pods_creation_batch_size = 1

# Permite que os usuários iniciem pods em vários namespaces.
# Vai exigir a criação de uma função de cluster para o agendador
multi_namespace_mode = False

# Use a conta de serviço que o kubernetes fornece aos pods para se conectar ao cluster do kubernetes.
# Destina-se a clientes que esperam rodar dentro de um pod rodando no kubernetes.
# Irá gerar uma exceção se for chamado de um processo que não está sendo executado em um ambiente kubernetes.
in_cluster = Verdadeiro

# Ao executar com in_cluster=False, altere o cluster_context ou config_file padrão
# opções para o cliente Kubernetes. Deixe em branco para usar o comportamento padrão como o ``kubectl`` tem.
# cluster_context =

# Caminho para o arquivo de configuração do kubernetes a ser usado quando ``in_cluster`` for definido como False
# arquivo_config =

# Parâmetros de palavras-chave a serem passados ​​ao chamar um cliente do kubernetes métodos core_v1_api
# do Kubernetes Executor fornecido como uma string de dicionário JSON formatada em uma única linha.
# A lista de parâmetros suportados é semelhante para todos os core_v1_apis, portanto, uma única configuração
# variável para todas as apis. Ver:
# https://raw.githubusercontent.com/kubernetes-client/python/41f11a09995efcd0142e25946adc7591431bfb2f/kubernetes/client/api/core_v1_api.py
kube_client_request_args =

# Argumentos de palavra-chave opcionais para passar para o cliente ``delete_namespaced_pod`` kubernetes
# método ``core_v1_api`` ao usar o Kubernetes Executor.
# Este deve ser um objeto e pode conter qualquer uma das opções listadas em ``v1DeleteOptions``
# classe definida aqui:
# https://github.com/kubernetes-client/python/blob/41f11a09995efcd0142e25946adc7591431bfb2f/kubernetes/client/models/v1_delete_options.py#L19
# Exemplo: delete_option_kwargs = {{"grace_period_seconds": 10}}
delete_option_kwargs =

# Habilita o mecanismo de manutenção de atividade TCP. Isso evita que as solicitações da API do Kubernetes sejam interrompidas indefinidamente
# quando a conexão ociosa atinge o tempo limite em serviços como balanceadores de carga de nuvem ou firewalls.
enable_tcp_keepalive = Verdadeiro

# Quando a opção `enable_tcp_keepalive` está habilitada, o TCP testa uma conexão que
# esteve ocioso por `tcp_keep_idle` segundos.
tcp_keep_idle = 120

# Quando a opção `enable_tcp_keepalive` estiver habilitada, se a API do Kubernetes não responder
# para um teste keepalive, o TCP retransmite o teste após `tcp_keep_intvl` segundos.
tcp_keep_intvl = 30

# Quando a opção `enable_tcp_keepalive` estiver habilitada, se a API do Kubernetes não responder
# para um teste keepalive, o TCP retransmite o teste `tcp_keep_cnt number` de vezes antes
# uma conexão é considerada quebrada.
tcp_keep_cnt = 6

# Defina como false para ignorar a verificação do certificado SSL do cliente python do Kubernetes.
verifique_ssl = Verdadeiro

# Quanto tempo em segundos um trabalhador pode estar em Pendente antes de ser considerado uma falha
worker_pods_pending_timeout = 300

# Quantas vezes em segundos para verificar se os trabalhadores pendentes excederam seus tempos limite
worker_pods_pending_timeout_check_interval = 120

# Com que frequência em segundos verificar instâncias de tarefas presas no status "enfileirado" sem um pod
worker_pods_queued_check_interval = 60

# Quantos pods pendentes devem ser verificados quanto a violações de tempo limite em cada intervalo de verificação.
# Você pode querer isso mais alto se tiver um cluster muito grande e/ou usar ``multi_namespace_mode``.
worker_pods_pending_timeout_batch_size = 100

[sensores]
# Tempo limite padrão do sensor, 7 dias por padrão (7 * 24 * 60 * 60).
default_timeout = 604800